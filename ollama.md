# Ollama
Secrets of ollama how ollama build and which technology used under the hood to perform its operation.


> `llama.cpp` or Ollama


### What is ineference?
to make predictions or generate outputs based on new input data.


* [LLM inference in C/C++](https://github.com/ggml-org/llama.cpp)
